// src/services/chunkedFileUploadService.ts

import { getStorage, ref, uploadBytes, getDownloadURL, deleteObject } from 'firebase/storage';
import { v4 as uuidv4 } from 'uuid';

export interface ChunkedUploadOptions {
  chunkSize?: number; // Default: 5MB per chunk
  maxRetries?: number; // Default: 3 retries per chunk
  timeoutMs?: number; // Default: 60 seconds per chunk
  onProgress?: (progress: ChunkedUploadProgress) => void;
  onChunkComplete?: (chunkIndex: number, totalChunks: number) => void;
}

export interface ChunkedUploadProgress {
  stage: 'preparing' | 'uploading' | 'assembling' | 'completed';
  percentage: number;
  currentChunk?: number;
  totalChunks?: number;
  bytesUploaded: number;
  totalBytes: number;
  currentOperation: string;
  estimatedTimeRemaining?: number;
}

export interface ChunkedUploadResult {
  url: string;
  path: string;
  name: string;
  type: string;
  size: number;
  uploadedAt: Date;
  chunks: number;
  uploadTime: number;
}

interface ChunkMetadata {
  index: number;
  size: number;
  start: number;
  end: number;
  uploadUrl?: string;
  uploadPath?: string;
  retries: number;
}

export class ChunkedFileUploadService {
  private storage = getStorage();
  private readonly DEFAULT_CHUNK_SIZE = 5 * 1024 * 1024; // 5MB
  private readonly DEFAULT_MAX_RETRIES = 3;
  private readonly DEFAULT_TIMEOUT = 60000; // 60 seconds

  /**
   * Upload large files using chunking strategy
   * Supports files up to 500MB (100 chunks √ó 5MB each)
   */
  async uploadLargeFile(
    file: File,
    storagePath: string,
    options: ChunkedUploadOptions = {}
  ): Promise<ChunkedUploadResult> {
    const startTime = Date.now();
    const {
      chunkSize = this.DEFAULT_CHUNK_SIZE,
      maxRetries = this.DEFAULT_MAX_RETRIES,
      timeoutMs = this.DEFAULT_TIMEOUT,
      onProgress,
      onChunkComplete
    } = options;

    // Validate file size (max 500MB)
    const maxFileSize = 500 * 1024 * 1024;
    if (file.size > maxFileSize) {
      throw new Error(`File size (${(file.size / 1024 / 1024).toFixed(1)}MB) exceeds maximum limit of 500MB`);
    }

    // Determine if chunking is needed
    const needsChunking = file.size > chunkSize;
    
    if (!needsChunking) {
      // Use standard upload for small files
      return this.uploadStandardFile(file, storagePath, onProgress);
    }

    console.log(`üöÄ Starting chunked upload for ${file.name} (${(file.size / 1024 / 1024).toFixed(1)}MB)`);

    onProgress?.({
      stage: 'preparing',
      percentage: 0,
      bytesUploaded: 0,
      totalBytes: file.size,
      currentOperation: 'Preparing file for chunked upload...'
    });

    // Create chunk metadata
    const chunks = this.createChunkMetadata(file, chunkSize);
    const sessionId = uuidv4();
    const chunkBasePath = `${storagePath}_chunks_${sessionId}`;

    onProgress?.({
      stage: 'preparing',
      percentage: 5,
      totalChunks: chunks.length,
      bytesUploaded: 0,
      totalBytes: file.size,
      currentOperation: `Created ${chunks.length} chunks for upload...`
    });

    try {
      // Upload all chunks
      await this.uploadChunks(file, chunks, chunkBasePath, {
        maxRetries,
        timeoutMs,
        onProgress: (chunkProgress) => {
          const overallProgress = this.calculateOverallProgress(chunks, chunkProgress);
          onProgress?.({
            stage: 'uploading',
            percentage: 5 + (overallProgress * 0.85), // 5-90% for upload
            currentChunk: chunkProgress.currentChunk,
            totalChunks: chunks.length,
            bytesUploaded: chunkProgress.bytesUploaded,
            totalBytes: file.size,
            currentOperation: chunkProgress.currentOperation,
            estimatedTimeRemaining: this.estimateTimeRemaining(startTime, overallProgress)
          });
        },
        onChunkComplete
      });

      onProgress?.({
        stage: 'assembling',
        percentage: 90,
        totalChunks: chunks.length,
        bytesUploaded: file.size,
        totalBytes: file.size,
        currentOperation: 'Assembling chunks into final file...'
      });

      // Assemble chunks into final file
      const finalResult = await this.assembleChunks(file, chunks, storagePath, sessionId);

      onProgress?.({
        stage: 'completed',
        percentage: 100,
        totalChunks: chunks.length,
        bytesUploaded: file.size,
        totalBytes: file.size,
        currentOperation: 'Upload completed successfully!'
      });

      const uploadTime = Date.now() - startTime;
      console.log(`‚úÖ Chunked upload completed in ${(uploadTime / 1000).toFixed(1)}s`);

      return {
        ...finalResult,
        chunks: chunks.length,
        uploadTime
      };

    } catch (error) {
      console.error('‚ùå Chunked upload failed:', error);
      // Clean up any uploaded chunks
      await this.cleanupFailedUpload(chunks);
      throw error;
    }
  }

  /**
   * Create chunk metadata for a file
   */
  private createChunkMetadata(file: File, chunkSize: number): ChunkMetadata[] {
    const chunks: ChunkMetadata[] = [];
    let start = 0;

    while (start < file.size) {
      const end = Math.min(start + chunkSize, file.size);
      chunks.push({
        index: chunks.length,
        size: end - start,
        start,
        end,
        retries: 0
      });
      start = end;
    }

    return chunks;
  }

  /**
   * Upload all chunks with retry logic and progress tracking
   */
  private async uploadChunks(
    file: File,
    chunks: ChunkMetadata[],
    basePath: string,
    options: {
      maxRetries: number;
      timeoutMs: number;
      onProgress?: (progress: { currentChunk: number; bytesUploaded: number; currentOperation: string }) => void;
      onChunkComplete?: (chunkIndex: number, totalChunks: number) => void;
    }
  ): Promise<void> {
    const { maxRetries, timeoutMs, onProgress, onChunkComplete } = options;
    let totalBytesUploaded = 0;

    // Upload chunks in batches to avoid overwhelming the browser
    const batchSize = 3;
    for (let i = 0; i < chunks.length; i += batchSize) {
      const batch = chunks.slice(i, Math.min(i + batchSize, chunks.length));
      
      // Process batch in parallel
      const batchPromises = batch.map(async (chunk) => {
        const chunkBlob = file.slice(chunk.start, chunk.end);
        const chunkPath = `${basePath}/chunk_${chunk.index.toString().padStart(4, '0')}`;
        
        let lastError: Error | null = null;
        
        for (let attempt = 0; attempt <= maxRetries; attempt++) {
          try {
            onProgress?.({
              currentChunk: chunk.index + 1,
              bytesUploaded: totalBytesUploaded,
              currentOperation: `Uploading chunk ${chunk.index + 1}/${chunks.length} (attempt ${attempt + 1})`
            });

            const result = await this.uploadSingleChunk(chunkBlob, chunkPath, timeoutMs);
            chunk.uploadUrl = result.url;
            chunk.uploadPath = result.path;
            
            totalBytesUploaded += chunk.size;
            onChunkComplete?.(chunk.index, chunks.length);
            
            console.log(`‚úÖ Chunk ${chunk.index + 1}/${chunks.length} uploaded successfully`);
            return;
            
          } catch (error) {
            lastError = error as Error;
            chunk.retries = attempt + 1;
            
            if (attempt < maxRetries) {
              console.warn(`‚ö†Ô∏è Chunk ${chunk.index + 1} failed (attempt ${attempt + 1}), retrying...`);
              // Exponential backoff
              await this.delay(Math.pow(2, attempt) * 1000);
            }
          }
        }
        
        throw new Error(`Failed to upload chunk ${chunk.index + 1} after ${maxRetries + 1} attempts: ${lastError?.message}`);
      });

      await Promise.all(batchPromises);
      
      // Brief pause between batches to prevent rate limiting
      if (i + batchSize < chunks.length) {
        await this.delay(500);
      }
    }
  }

  /**
   * Upload a single chunk with timeout
   */
  private async uploadSingleChunk(
    chunkBlob: Blob,
    chunkPath: string,
    timeoutMs: number
  ): Promise<{ url: string; path: string }> {
    const storageRef = ref(this.storage, chunkPath);
    
    const uploadPromise = async () => {
      const snapshot = await uploadBytes(storageRef, chunkBlob);
      const downloadURL = await getDownloadURL(snapshot.ref);
      return {
        url: downloadURL,
        path: snapshot.ref.fullPath
      };
    };

    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(() => reject(new Error(`Chunk upload timeout after ${timeoutMs}ms`)), timeoutMs);
    });

    return Promise.race([uploadPromise(), timeoutPromise]);
  }

  /**
   * Assemble chunks into the final file
   */
  private async assembleChunks(
    originalFile: File,
    chunks: ChunkMetadata[],
    finalPath: string,
    sessionId: string
  ): Promise<{ url: string; path: string; name: string; type: string; size: number; uploadedAt: Date }> {
    // For Firebase Storage, we need to create a single file from all chunks
    // Since Firebase doesn't support server-side chunk assembly, we'll reconstruct locally
    
    const assembledBlob = await this.reconstructFileFromChunks(originalFile, chunks);
    const finalStorageRef = ref(this.storage, finalPath);
    
    const snapshot = await uploadBytes(finalStorageRef, assembledBlob);
    const downloadURL = await getDownloadURL(snapshot.ref);
    
    // Clean up chunk files
    await this.cleanupChunks(chunks);
    
    return {
      url: downloadURL,
      path: snapshot.ref.fullPath,
      name: originalFile.name,
      type: originalFile.type,
      size: originalFile.size,
      uploadedAt: new Date()
    };
  }

  /**
   * Reconstruct file from uploaded chunks (fallback for Firebase)
   */
  private async reconstructFileFromChunks(
    originalFile: File,
    chunks: ChunkMetadata[]
  ): Promise<Blob> {
    // Since we can't download chunks from Firebase efficiently,
    // we'll use the original file slicing approach
    const chunkBlobs: Blob[] = [];
    
    for (const chunk of chunks.sort((a, b) => a.index - b.index)) {
      chunkBlobs.push(originalFile.slice(chunk.start, chunk.end));
    }
    
    return new Blob(chunkBlobs, { type: originalFile.type });
  }

  /**
   * Standard upload for smaller files
   */
  private async uploadStandardFile(
    file: File,
    storagePath: string,
    onProgress?: (progress: ChunkedUploadProgress) => void
  ): Promise<ChunkedUploadResult> {
    const startTime = Date.now();
    const storageRef = ref(this.storage, storagePath);
    
    onProgress?.({
      stage: 'uploading',
      percentage: 10,
      bytesUploaded: 0,
      totalBytes: file.size,
      currentOperation: 'Uploading file...'
    });

    const snapshot = await uploadBytes(storageRef, file);
    
    onProgress?.({
      stage: 'uploading',
      percentage: 90,
      bytesUploaded: file.size,
      totalBytes: file.size,
      currentOperation: 'Getting download URL...'
    });

    const downloadURL = await getDownloadURL(snapshot.ref);
    
    onProgress?.({
      stage: 'completed',
      percentage: 100,
      bytesUploaded: file.size,
      totalBytes: file.size,
      currentOperation: 'Upload completed!'
    });

    const uploadTime = Date.now() - startTime;

    return {
      url: downloadURL,
      path: snapshot.ref.fullPath,
      name: file.name,
      type: file.type,
      size: file.size,
      uploadedAt: new Date(),
      chunks: 1,
      uploadTime
    };
  }

  /**
   * Calculate overall progress from chunk progress
   */
  private calculateOverallProgress(chunks: ChunkMetadata[], chunkProgress: any): number {
    const completedChunks = chunks.filter(c => c.uploadUrl).length;
    const currentChunkProgress = chunkProgress.currentChunk ? 
      (chunkProgress.bytesUploaded / chunks.reduce((sum, c) => sum + c.size, 0)) : 0;
    
    return (completedChunks + currentChunkProgress) / chunks.length;
  }

  /**
   * Estimate time remaining based on current progress
   */
  private estimateTimeRemaining(startTime: number, progress: number): number {
    if (progress <= 0) return 0;
    
    const elapsed = Date.now() - startTime;
    const estimatedTotal = elapsed / progress;
    return Math.max(0, estimatedTotal - elapsed);
  }

  /**
   * Clean up uploaded chunks
   */
  private async cleanupChunks(chunks: ChunkMetadata[]): Promise<void> {
    const cleanupPromises = chunks
      .filter(chunk => chunk.uploadPath)
      .map(async (chunk) => {
        try {
          const chunkRef = ref(this.storage, chunk.uploadPath!);
          await deleteObject(chunkRef);
        } catch (error) {
          console.warn(`Failed to cleanup chunk ${chunk.index}:`, error);
        }
      });

    await Promise.allSettled(cleanupPromises);
  }

  /**
   * Clean up after failed upload
   */
  private async cleanupFailedUpload(chunks: ChunkMetadata[]): Promise<void> {
    console.log('üßπ Cleaning up failed chunked upload...');
    await this.cleanupChunks(chunks);
  }

  /**
   * Utility function for delays
   */
  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * Validate file for chunked upload
   */
  validateFileForChunkedUpload(file: File): { isValid: boolean; error?: string } {
    const maxSize = 500 * 1024 * 1024; // 500MB
    if (file.size > maxSize) {
      return {
        isValid: false,
        error: `File size (${(file.size / 1024 / 1024).toFixed(1)}MB) exceeds maximum limit of 500MB`
      };
    }

    const allowedTypes = [
      'application/vnd.openxmlformats-officedocument.presentationml.presentation', // .pptx
      'application/vnd.ms-powerpoint', // .ppt
      'application/pdf', // .pdf
      'application/vnd.openxmlformats-officedocument.wordprocessingml.document', // .docx
      'application/msword', // .doc
      'text/plain' // .txt
    ];

    if (!allowedTypes.includes(file.type)) {
      return {
        isValid: false,
        error: `Unsupported file type: ${file.type}. Supported formats: PowerPoint, PDF, Word, Text files`
      };
    }

    return { isValid: true };
  }
}

// Singleton instance
export const chunkedFileUploadService = new ChunkedFileUploadService();
